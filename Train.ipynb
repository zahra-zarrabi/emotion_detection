{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zahra-zarrabi/emotion_detection/blob/main/Train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFafKuUt95j0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ead6926-e232-4e82-e977-1de342fb5112"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNV_WKDmcB1R",
        "outputId": "931e3cff-c804-438b-ea16-4bad72b150e0"
      },
      "source": [
        "%cd \"/content/drive/MyDrive/Emotion-detection\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1u36KYGMcr7PbW3DSjuZPv6qQTGBnkAfX/Emotion-detection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8IzNPgXbReT"
      },
      "source": [
        "\n",
        "!gdown https://drive.google.com/uc?id=1X60B-uR3NtqPd4oosdotpbDgy8KOfUdr"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e3yqQLbRbQ6A"
      },
      "source": [
        "!unzip -d \"src/\" \"data.zip\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ym3XADSlcXVg",
        "outputId": "7e1c1c70-633c-4071-97a8-ab9e32d7cadd"
      },
      "source": [
        "%cd 'src/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/.shortcut-targets-by-id/1u36KYGMcr7PbW3DSjuZPv6qQTGBnkAfX/Emotion-detection/src\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FpXAa5DW-UER",
        "outputId": "6de35c89-6580-423d-c31e-b94ed703dd6a"
      },
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Flatten\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import os\n",
        "from keras.callbacks import EarlyStopping\n",
        "import keras\n",
        "\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# plots accuracy and loss curves\n",
        "def plot_model_history(model_history):\n",
        "    \"\"\"\n",
        "    Plot Accuracy and Loss curves given the model_history\n",
        "    \"\"\"\n",
        "    fig, axs = plt.subplots(1,2,figsize=(15,5))\n",
        "    # summarize history for accuracy\n",
        "    axs[0].plot(range(1,len(model_history.history['accuracy'])+1),model_history.history['accuracy'])\n",
        "    axs[0].plot(range(1,len(model_history.history['val_accuracy'])+1),model_history.history['val_accuracy'])\n",
        "    axs[0].set_title('Model Accuracy')\n",
        "    axs[0].set_ylabel('Accuracy')\n",
        "    axs[0].set_xlabel('Epoch')\n",
        "    axs[0].set_xticks(np.arange(1,len(model_history.history['accuracy'])+1),len(model_history.history['accuracy'])/10)\n",
        "    axs[0].legend(['train', 'val'], loc='best')\n",
        "    # summarize history for loss\n",
        "    axs[1].plot(range(1,len(model_history.history['loss'])+1),model_history.history['loss'])\n",
        "    axs[1].plot(range(1,len(model_history.history['val_loss'])+1),model_history.history['val_loss'])\n",
        "    axs[1].set_title('Model Loss')\n",
        "    axs[1].set_ylabel('Loss')\n",
        "    axs[1].set_xlabel('Epoch')\n",
        "    axs[1].set_xticks(np.arange(1,len(model_history.history['loss'])+1),len(model_history.history['loss'])/10)\n",
        "    axs[1].legend(['train', 'val'], loc='best')\n",
        "    fig.savefig('plot.png')\n",
        "    plt.show()\n",
        "\n",
        "# Define data generators\n",
        "train_dir = 'data/train'\n",
        "val_dir = 'data/test'\n",
        "\n",
        "num_train = 28709\n",
        "num_val = 7178\n",
        "batch_size = 64\n",
        "num_epoch = 50\n",
        "\n",
        "train_datagen = ImageDataGenerator(rescale=1./255)\n",
        "val_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "        train_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "validation_generator = val_datagen.flow_from_directory(\n",
        "        val_dir,\n",
        "        target_size=(48,48),\n",
        "        batch_size=batch_size,\n",
        "        color_mode=\"grayscale\",\n",
        "        class_mode='categorical')\n",
        "\n",
        "# Create the model\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(48,48,1)))\n",
        "model.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(1024, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(7, activation='softmax'))\n",
        "\n",
        "# Save checkpoints\n",
        "checkpoint = keras.callbacks.ModelCheckpoint('model_em/model{epoch:08d}.h5',period=1)\n",
        "early_stop = EarlyStopping(monitor='val_loss',restore_best_weights=True,patience=6,mode='min')\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',optimizer=Adam(lr=0.0001, decay=1e-6),metrics=['accuracy'])\n",
        "model_info = model.fit_generator(\n",
        "        train_generator,\n",
        "        steps_per_epoch=num_train // batch_size,\n",
        "        epochs=num_epoch,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=num_val // batch_size,\n",
        "        callbacks=[checkpoint,early_stop])\n",
        "\n",
        "plot_model_history(model_info)\n",
        "model.save_weights('model.h5')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:105: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "448/448 [==============================] - 9736s 22s/step - loss: 1.8085 - accuracy: 0.2578 - val_loss: 1.7494 - val_accuracy: 0.3278\n",
            "Epoch 2/50\n",
            "448/448 [==============================] - 380s 848ms/step - loss: 1.6402 - accuracy: 0.3596 - val_loss: 1.5528 - val_accuracy: 0.4120\n",
            "Epoch 3/50\n",
            "448/448 [==============================] - 379s 845ms/step - loss: 1.5373 - accuracy: 0.4085 - val_loss: 1.4718 - val_accuracy: 0.4332\n",
            "Epoch 4/50\n",
            "448/448 [==============================] - 369s 824ms/step - loss: 1.4683 - accuracy: 0.4368 - val_loss: 1.4128 - val_accuracy: 0.4626\n",
            "Epoch 5/50\n",
            "448/448 [==============================] - 374s 834ms/step - loss: 1.4098 - accuracy: 0.4635 - val_loss: 1.3667 - val_accuracy: 0.4798\n",
            "Epoch 6/50\n",
            "448/448 [==============================] - 376s 838ms/step - loss: 1.3610 - accuracy: 0.4838 - val_loss: 1.3270 - val_accuracy: 0.4985\n",
            "Epoch 7/50\n",
            "448/448 [==============================] - 376s 840ms/step - loss: 1.3205 - accuracy: 0.5005 - val_loss: 1.2907 - val_accuracy: 0.5109\n",
            "Epoch 8/50\n",
            "448/448 [==============================] - 374s 834ms/step - loss: 1.2747 - accuracy: 0.5218 - val_loss: 1.2646 - val_accuracy: 0.5209\n",
            "Epoch 9/50\n",
            "448/448 [==============================] - 371s 829ms/step - loss: 1.2392 - accuracy: 0.5322 - val_loss: 1.2337 - val_accuracy: 0.5258\n",
            "Epoch 10/50\n",
            "448/448 [==============================] - 372s 831ms/step - loss: 1.2101 - accuracy: 0.5421 - val_loss: 1.2079 - val_accuracy: 0.5385\n",
            "Epoch 11/50\n",
            "448/448 [==============================] - 378s 843ms/step - loss: 1.1731 - accuracy: 0.5630 - val_loss: 1.1922 - val_accuracy: 0.5439\n",
            "Epoch 12/50\n",
            "448/448 [==============================] - 373s 833ms/step - loss: 1.1493 - accuracy: 0.5707 - val_loss: 1.1671 - val_accuracy: 0.5587\n",
            "Epoch 13/50\n",
            "448/448 [==============================] - 366s 816ms/step - loss: 1.1189 - accuracy: 0.5824 - val_loss: 1.1550 - val_accuracy: 0.5632\n",
            "Epoch 14/50\n",
            "448/448 [==============================] - 365s 814ms/step - loss: 1.0948 - accuracy: 0.5905 - val_loss: 1.1487 - val_accuracy: 0.5619\n",
            "Epoch 15/50\n",
            "448/448 [==============================] - 367s 818ms/step - loss: 1.0659 - accuracy: 0.6045 - val_loss: 1.1384 - val_accuracy: 0.5721\n",
            "Epoch 16/50\n",
            "448/448 [==============================] - 372s 829ms/step - loss: 1.0414 - accuracy: 0.6136 - val_loss: 1.1228 - val_accuracy: 0.5767\n",
            "Epoch 17/50\n",
            "448/448 [==============================] - 371s 828ms/step - loss: 1.0215 - accuracy: 0.6216 - val_loss: 1.1120 - val_accuracy: 0.5813\n",
            "Epoch 18/50\n",
            "448/448 [==============================] - 366s 817ms/step - loss: 0.9989 - accuracy: 0.6330 - val_loss: 1.1027 - val_accuracy: 0.5893\n",
            "Epoch 19/50\n",
            "448/448 [==============================] - 371s 828ms/step - loss: 0.9762 - accuracy: 0.6385 - val_loss: 1.1036 - val_accuracy: 0.5879\n",
            "Epoch 20/50\n",
            "448/448 [==============================] - 372s 830ms/step - loss: 0.9466 - accuracy: 0.6508 - val_loss: 1.0946 - val_accuracy: 0.5904\n",
            "Epoch 21/50\n",
            "448/448 [==============================] - 358s 799ms/step - loss: 0.9203 - accuracy: 0.6596 - val_loss: 1.0943 - val_accuracy: 0.5963\n",
            "Epoch 22/50\n",
            "448/448 [==============================] - 360s 803ms/step - loss: 0.9005 - accuracy: 0.6686 - val_loss: 1.1044 - val_accuracy: 0.5910\n",
            "Epoch 23/50\n",
            "448/448 [==============================] - 367s 819ms/step - loss: 0.8791 - accuracy: 0.6751 - val_loss: 1.0856 - val_accuracy: 0.5975\n",
            "Epoch 24/50\n",
            "448/448 [==============================] - 371s 827ms/step - loss: 0.8581 - accuracy: 0.6833 - val_loss: 1.0780 - val_accuracy: 0.5956\n",
            "Epoch 25/50\n",
            "448/448 [==============================] - 367s 818ms/step - loss: 0.8273 - accuracy: 0.6945 - val_loss: 1.0776 - val_accuracy: 0.6044\n",
            "Epoch 26/50\n",
            "448/448 [==============================] - 364s 813ms/step - loss: 0.8071 - accuracy: 0.7047 - val_loss: 1.0887 - val_accuracy: 0.6059\n",
            "Epoch 27/50\n",
            "448/448 [==============================] - 366s 816ms/step - loss: 0.7885 - accuracy: 0.7138 - val_loss: 1.0853 - val_accuracy: 0.6042\n",
            "Epoch 28/50\n",
            "448/448 [==============================] - 360s 803ms/step - loss: 0.7687 - accuracy: 0.7183 - val_loss: 1.0870 - val_accuracy: 0.6090\n",
            "Epoch 29/50\n",
            "448/448 [==============================] - 364s 813ms/step - loss: 0.7483 - accuracy: 0.7267 - val_loss: 1.0868 - val_accuracy: 0.6091\n",
            "Epoch 30/50\n",
            " 80/448 [====>.........................] - ETA: 4:37 - loss: 0.7151 - accuracy: 0.7383"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glPyKj9R-Zia"
      },
      "source": [
        "model.save('model_emotion.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}